{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jira Analysis\n",
    "\n",
    "This notebook contains scripts to process a set of tickets dumped from Jira using a LangChain chain and Pinecone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS CODE DOES NOT RUN YET!!\n",
    "\n",
    "## Imports\n",
    "\n",
    "I hate having imports strewn all over the code and so, I'm creating a section where I'll keep adding imports.\n",
    "\n",
    "I recognize that this requires me to do a \"Run All\" in the Notebook each time, but it's better that than import hell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "from uuid import uuid4\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.read_csv(\"jira_csv.csv\")\n",
    "\n",
    "# Let's make sure we don't have an empty description\n",
    "df.Description = df.Description.fillna(\"No Description Available\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Andre:\\n\\nApp: [https://app.crowdbotics.com/dashboard/app/39571|https://app.crowdbotics.com/dashboard/app/39571]\\nI created a Connector to test OpenAPI response and as it’s an authenticated request, I added the Bearer Token. It looks like an EnvVar was added to the api.js file but when trying to deploy, it failed (stack trace is the first message in the app Activity Log) because of the newly added token.\\n\\nIssue with react-native-dotenv: [https://app.circleci.com/pipelines/github/crowdbotics-apps/andre-test-mar-27-39571/4/workflows/c8bdd4fc-a811-4993-afe7-7a610591a870/jobs/12|https://app.circleci.com/pipelines/github/crowdbotics-apps/andre-test-mar-27-39571/4/workflows/c8bdd4fc-a811-4993-afe7-7a610591a870/jobs/12]\\n\\n\\nAnother issue is regarding the connector code generated in GitHub. When I first added the connector, and it has a token, the token was correctly added to the openAPI store, but after changing the connector detail to add a few more fields to the response and save, the token was removed from the code. I had to go back to the connector and add the token again and save, then it was added back to the connector’s store.\\n\\n----\\n\\nSteps to test and reproduce\\n\\n# Go to Connectors page\\n# Create a connector with Bearer auth (can be fake information)\\n# Save\\n# Check if env var is added to the connector code like this: [https://github.com/crowdbotics-dev/aline-032923-dev-73007/blob/323de66db33c0ccd349eb64c10a0bf33958c89cc/store/rapidAPICocktails/api.js#L8|https://github.com/crowdbotics-dev/aline-032923-dev-73007/blob/323de66db33c0ccd349eb64c10a0bf33958c89cc/store/rapidAPICocktails/api.js#L8|smart-link] \\n# Go to \"Active in my project\" tab\\n# Edit the connector (like the description or new data call) - but do not edit the auth token. Save\\n# Expect the Bearer header to still exist in the connector generated code.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Description\n",
       "3  From Andre:\\n\\nApp: [https://app.crowdbotics.com/dashboard/app/39571|https://app.crowdbotics.com/dashboard/app/39571]\\nI created a Connector to test OpenAPI response and as it’s an authenticated request, I added the Bearer Token. It looks like an EnvVar was added to the api.js file but when trying to deploy, it failed (stack trace is the first message in the app Activity Log) because of the newly added token.\\n\\nIssue with react-native-dotenv: [https://app.circleci.com/pipelines/github/crowdbotics-apps/andre-test-mar-27-39571/4/workflows/c8bdd4fc-a811-4993-afe7-7a610591a870/jobs/12|https://app.circleci.com/pipelines/github/crowdbotics-apps/andre-test-mar-27-39571/4/workflows/c8bdd4fc-a811-4993-afe7-7a610591a870/jobs/12]\\n\\n\\nAnother issue is regarding the connector code generated in GitHub. When I first added the connector, and it has a token, the token was correctly added to the openAPI store, but after changing the connector detail to add a few more fields to the response and save, the token was removed from the code. I had to go back to the connector and add the token again and save, then it was added back to the connector’s store.\\n\\n----\\n\\nSteps to test and reproduce\\n\\n# Go to Connectors page\\n# Create a connector with Bearer auth (can be fake information)\\n# Save\\n# Check if env var is added to the connector code like this: [https://github.com/crowdbotics-dev/aline-032923-dev-73007/blob/323de66db33c0ccd349eb64c10a0bf33958c89cc/store/rapidAPICocktails/api.js#L8|https://github.com/crowdbotics-dev/aline-032923-dev-73007/blob/323de66db33c0ccd349eb64c10a0bf33958c89cc/store/rapidAPICocktails/api.js#L8|smart-link] \\n# Go to \"Active in my project\" tab\\n# Edit the connector (like the description or new data call) - but do not edit the auth token. Save\\n# Expect the Bearer header to still exist in the connector generated code."
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_col = df.columns.get_loc('Description')\n",
    "\n",
    "# I use the 4th record in the list because it has a really long description.\n",
    "df.iloc[[3],[desc_col]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk the data\n",
    "\n",
    "In our case, we will only chunk up the `Description` field from Jira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    '''\n",
    "    Creates tokens from input text and returns the number of tokens.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The text to be tokenized\n",
    "        \n",
    "        Returns:\n",
    "            The number of tokens created from the text (int)\n",
    "    '''\n",
    "    tokens = tokenizer.encode(text, disallowed_special=())\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way of finding the number of tokens, let us initialize a splitter that uses the `tiktoken_len` function that we just created to split input text so that each chunk is never larger than a maximum that we set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the OpenAI API and create a test embedding just so we know everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "###### TEST TO MAKE SURE OPENAI API KEY WORKS\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "len(res), len(res[0])\n",
    "###### END TEST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to create and initialize our vector database using Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"langchain-retrieval-augmentation\"\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=len(res[0])  # 1536 dim of text-embedding-ada-002\n",
    "    )\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a0bb2fb5714cc4992b534fb1c20f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 3; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m embeds \u001b[39m=\u001b[39m embed\u001b[39m.\u001b[39membed_documents(descriptions)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(ids), \u001b[39mlen\u001b[39m(descriptions), \u001b[39mlen\u001b[39m(metadata_list), \u001b[39mlen\u001b[39m(embeds))\n\u001b[1;32m---> 32\u001b[0m upsert_vectors \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(ids, embeds, metadata_list))\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(upsert_vectors)\n\u001b[0;32m     34\u001b[0m index\u001b[39m.\u001b[39mupsert(vectors\u001b[39m=\u001b[39mupsert_vectors)\n",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 3; 2 is required"
     ]
    }
   ],
   "source": [
    "batch_limit = 100\n",
    "\n",
    "descriptions = [] # list to store chunked descriptions\n",
    "metadata_list = []\n",
    "\n",
    "data_list = df.to_dict(\"records\")   # converts df to list of dicts\n",
    "                                    # Makes it easier to iterate.\n",
    "\n",
    "for i,row in enumerate(tqdm(data_list)):\n",
    "    metadata = {\n",
    "        \"id\": row[\"Issue key\"],\n",
    "        \"type\": row[\"Issue Type\"],\n",
    "        \"status\": row[\"Status\"],\n",
    "        \"summary\": row[\"Summary\"],\n",
    "        \"created\": row[\"Created\"],\n",
    "        \"resolved\": row[\"Resolved\"]\n",
    "    }\n",
    "\n",
    "    # Create chunks for description of each row\n",
    "    row_chunks = text_splitter.split_text(row['Description'])\n",
    "    # Create metadata for each chunk\n",
    "    metadata_chunks = [{\n",
    "        \"chunk\": j, **metadata\n",
    "    } for j, _ in enumerate(row_chunks)]\n",
    "    descriptions.extend(row_chunks)\n",
    "    metadata_list.extend(metadata_chunks)\n",
    "    # Loop until you've reached the batch limit\n",
    "    if len(descriptions) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(descriptions))]\n",
    "        embeds = embed.embed_documents(descriptions)\n",
    "        print(len(ids), len(descriptions), len(metadata_list), len(embeds))\n",
    "        upsert_vectors = dict(zip(ids, embeds, metadata_list))\n",
    "        print(upsert_vectors)\n",
    "        index.upsert(vectors=upsert_vectors)\n",
    "        descriptions = []\n",
    "        metadata_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jira-analysis-41397-YTsRMCK2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
